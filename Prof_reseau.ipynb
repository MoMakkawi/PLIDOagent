{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b198ef-1cce-4e75-a49b-a129fb20f8f6",
   "metadata": {},
   "source": [
    "<img src=\"images/network_prof.png\" width=\"150\" alt=\"Prof looking at a electronic brain\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\"> Dans cette partie, nous allons cr√©er de vrais agents pour r√©pondre aux questions des √©l√®ves sur le contenu du cours r√©seau. \n",
    "\n",
    "Avec le professeur de philosophie, nous avons vu les interrogrations de base d'un LLM, avec des appels de bas niveau.\n",
    "Cela demandait pas mal de code, et surtout les interrogrations √©taient s√©quentielles. Quand nous avons demander √† plusieurs LLM de plancer sur le devoir, il a fallu attendre la r√©ponse\n",
    "de la premi√®re pour lancer la seconde, alors qu'elles auraient faire ce travail en m√™me temps.\n",
    "\n",
    "Voici la liste des modules dont nous aurons besoin dans cette partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c890d334-bf11-46ce-ad5c-c3445ccdbbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "from IPython.display import Markdown, display \n",
    "\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from agents import Agent, Runner, trace, function_tool, OpenAIChatCompletionsModel, input_guardrail, GuardrailFunctionOutput\n",
    "\n",
    "import gradio\n",
    "import asyncio\n",
    "import requests \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0c84e-acd3-424b-ba57-6e8a8ab0e831",
   "metadata": {},
   "source": [
    "## Retrouver le contenu du livre \"Programmer l'Internet des Objets\"\n",
    "\n",
    "Dans un premier temps, nous allons convertir en texte les premi√®res pages du livre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12884f1a-3e0f-4431-92ee-5a9c36b189aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book = PdfReader(\"./PLIDO_BOOK_en.pdf\")\n",
    "book_content = \"\"\n",
    "for page in book.pages:\n",
    "    text = page.extract_text()\n",
    "    book_content += text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d8d52-7ddc-4572-8e7f-3e382c75119d",
   "metadata": {},
   "source": [
    "## Donner l'information au LLM\n",
    "\n",
    "<img src=\"images/agent.png\" width=\"150\" alt=\"One agent\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\">Une fois les cl√©s API charg√©es pour plusieurs serveurs (on utilisera par d√©faut celui de l'Universit√© de Rennes, mais l'utilisation de Gemini est aussi possible). On donne l'URI et le Token pour le service, puis dans un deuxi√®me temps, on pr√©cise le mod√®le LLM utilis√©. Si on utilisait par defaut OpenAI, ces lignes seraient inutiles. Lors de l'appel d'```Agent``` , dans le cas d'OpenAI, le nom du mod√®le est directement indiqu√© dans une cha√Æne de caract√®res. \n",
    "\n",
    "A la cr√©ation de l'Agent, lui donne un identifiant pour les traces, puis les instructions qui vont lui indiquer son r√¥le, les limites des r√©ponses et les actions qu'il devra faire dans certain cas. Ces instructions contiennent √©galement l'integralit√© du livre en ASCII. A noter que l'on demande au LLM de ne pas chercher √† r√©pondre en d√©tail si la r√©ponse ne se trouve pas dans le livre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3baea2-a9a5-47a9-94bb-e4b5fcd978c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "rennes_api_key = os.getenv(\"RENNES_API_KEY\")\n",
    "if not rennes_api_key:\n",
    "    print(\"RENNES_API_KEY is missing\")\n",
    "    exit(1)\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not rennes_api_key:\n",
    "    print(\"GOOLE_API_KEY is missing\")\n",
    "    exit(1)\n",
    "    \n",
    "RENNES_BASE_URL = \"https://ragarenn.eskemm-numerique.fr/sso/ch@t/api\"\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "rennes_client = AsyncOpenAI(base_url=RENNES_BASE_URL, api_key=rennes_api_key)\n",
    "rennes_model  = OpenAIChatCompletionsModel(model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", openai_client=rennes_client)\n",
    "\n",
    "gemini_client = AsyncOpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "gemini_model  = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=gemini_client)\n",
    "\n",
    "ollama_client = AsyncOpenAI(base_url=OLLAMA_BASE_URL, api_key=\"dont matter\")\n",
    "ollama_model  = OpenAIChatCompletionsModel(model=\"gemma3:1b\", openai_client=ollama_client)\n",
    "\n",
    "\n",
    "instructions = f\"\"\"Voici le contenu d'un livre sur l'Internet des Objets\n",
    "\n",
    "{book_content}\n",
    "\n",
    "Ta responsabilit√© est de repr√©senter l'auteur (Laurent Toutain) pour des interactions avec les √©l√®ves.\n",
    "Les r√©ponses doivent √™tre professionnelles, claires, et doivent donner envie aux √©tudiants de s'engager\n",
    "dans le cours, voire de choisir cette formation.\n",
    "Si tu ne connais pas la r√©ponse aux questions, r√©pond Non.\"\"\"\n",
    "\n",
    "book_agent = Agent(name=\"PLIDO Book Agent\", instructions=instructions, model=gemini_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7e563-0477-4dd9-8e7b-d8f8faddfda5",
   "metadata": {},
   "source": [
    "Un agent se lance en utilisant la coroutine Python ```run``` du module ```Runner``` import√© du module ```agents``` d'OpenAI.  L'utilisation du mot-cl√© ```await``` est indispensable. Ici, la diff√©rence avec une fonction est minime, vu que l'on ne lance qu'une coroutine et que l'on attend sa fin pour passer √† l'instruction suivante. \n",
    "\n",
    "Vous pouvez relancer la cellule suivante plusieurs fois en changeant la question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3883e9ec-d187-4389-8e5b-032482789221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Bonjour !\n",
       "\n",
       "C'est une excellente question ! \"Programming the Internet of Things\" est un livre riche en informations et en concepts techniques. Sa densit√© pourrait le rendre plus adapt√© √† une lecture attentive √† un bureau ou dans un environnement calme, o√π vous pouvez prendre des notes et exp√©rimenter avec les exemples.\n",
       "\n",
       "Cela dit, si vous √™tes passionn√© par l'IoT et que vous aimez plonger dans des sujets techniques m√™me en vacances, pourquoi pas ? üòä Assurez-vous d'avoir de quoi prendre des notes et peut-√™tre un appareil pour tester quelques id√©es si l'envie vous prend.\n",
       "\n",
       "En r√©sum√© :\n",
       "*   **Pour :** Si vous aimez les d√©fis techniques et apprendre m√™me en vacances.\n",
       "*   **Contre :** Si vous cherchez une lecture l√©g√®re et facile pour vous d√©tendre compl√®tement.\n",
       "\n",
       "J'esp√®re que cela vous aide √† prendre votre d√©cision !"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await Runner.run(book_agent, \"Est ce que le livre est un bon livre √† lire √† la plage?\")\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "487fe2be-936b-40a6-acfe-3dc899c14f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def chat_async(message, history):\n",
    "    \"\"\"Fonction async pour l'agent\"\"\"\n",
    "    try:\n",
    "        result = await Runner.run(book_agent, message)\n",
    "        return result.final_output\n",
    "    except Exception as e:\n",
    "        return f\"Erreur: {e}\"\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Fonction sync pour ChatInterface\"\"\"\n",
    "    return asyncio.run(chat_async(message, history)) \n",
    "\n",
    "gradio.ChatInterface(chat_fn, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8179ef0-93d9-4a54-b2cd-39e9e349d92a",
   "metadata": {},
   "source": [
    "On peut voir que cela am√®ne √† quelques acrobaties dans Python. Gradio appelle une fonction en callback pour traiter la commande de l'utilisateur, et l'interaction avec l'agent se fait par une coroutine. D'o√π la fonction `chat_fn` qui ne fait qu'appeler la coroutine, attend la fin de son execution gr√¢ce √† `asyncio.run` et retourne le r√©sultat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042cd5f-110b-4520-84b5-96afb4f5359a",
   "metadata": {},
   "source": [
    "# Plusieurs agents \n",
    "\n",
    "<img src=\"images/agents.png\" width=\"150\" alt=\"Several agents\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\"> Nous allons reprendre une structure classique pour voir comment exploiter le parallisme avec `asyncio`, nous allons demander √† deux LLM de cogiter sur la m√™me question et l'on prendra la meilleure des deux. Comme nous allons utiliser le mod√®le ollama en local, il y a peu de chance qu'elle fasse la r√©ponse la plus fut√©e, mais sait-on jamais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08102273-53f0-414d-9d1d-a0499784119d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_agent = Agent(name=\"PLIDO Book Agent by Ollama\", instructions=instructions, model=ollama_model)\n",
    "\n",
    "instructions= \"\"\"selectionne la r√©ponse la plus claire parmi les diff√©rentes options √† ces questions\n",
    "d'√©tudiants sur un cours. Il faut prendre celui qui te deonnera le plus envie de suivre les cours.\n",
    "Ne rajoute pas d'explication aux r√©ponses possibles. Repond juste avec la meilleure r√©ponse\"\"\"\n",
    "\n",
    "best_answer  = Agent(name=\"Response selection\", instructions=instructions, model=gemini_model)\n",
    "\n",
    "async def chat_async(message, history):\n",
    "    \"\"\"Fonction async pour les appels au deux agents puis √† la s√©lection\"\"\"\n",
    "    \n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(book_agent, message),\n",
    "        Runner.run(ollama_agent, message),\n",
    "    )\n",
    "    outputs = [result.final_output for result in results]\n",
    "\n",
    "    answers = \"R√©ponses √† la question:\\n\\n\" + \"\\nR√©ponse:\\n\".join(outputs)\n",
    "    best = await Runner.run(best_answer, answers)\n",
    "\n",
    "    return best.final_output\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Fonction sync pour ChatInterface\"\"\"\n",
    "    return asyncio.run(chat_async(message, history)) \n",
    "\n",
    "gradio.ChatInterface(chat_fn, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60809034-330a-41d2-904b-c91ca3c0d1d5",
   "metadata": {},
   "source": [
    "# Envoi de message\n",
    "<img src=\"images/telephone.png\" width=\"150\" alt=\"Several agents\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\">\n",
    "Nous pouvons demander au LLM d'interagir avec le monde ext√©rieur. pour cela nous allons utiliser une application qui envoie des messages sur votre t√©l√©phone portable.\n",
    "\n",
    "* t√©l√©chargez depuis votre magasin d'application (Android ou Apple) l'application `pushover`\n",
    "* Cr√©ez votre compte\n",
    "* Loguez vous √©galement depuis votre ordinateur\n",
    "* Recup√©rez votre cl√© d'utilisateur qui apparait en haut √† droite et stockez l√† dans la la variable `PUSHOVER_USER_ID` dans le fichier `.env`\n",
    "* En bas de la page, choisissez *Your Applications* et cliquez sur *Create an Application/Token*\n",
    "  * Donnez un nom comme *PLIDOagent*, et une fois valid√©, un token va appara√Ætre\n",
    "  * Mettez ce token dans la variable `PUSHOVER_TOKEN` dans le fichier `.env`\n",
    "\n",
    "Le petit programme ci dessus vous permet de teste si ca marche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a75c7f9-d628-4951-9b18-32690ea0cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushover_token=os.getenv(\"PUSHOVER_TOKEN\")\n",
    "pushover_user_id=os.getenv(\"PUSHOVER_USER_ID\")\n",
    "pushover_uri =\"https://api.pushover.net/1/messages.json\"\n",
    "\n",
    "if not pushover_token or not pushover_user_id:\n",
    "    print (\"User_id or token missing\")\n",
    "    exit(1)\n",
    "\n",
    "def push(message):\n",
    "    payload = {\"user\": pushover_user_id, \"token\": pushover_token, \"message\": message}\n",
    "    x = requests.post(pushover_uri, data=payload)\n",
    "\n",
    "push(\"Hello my dear\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99f4dd-e4ad-439c-b44e-d92f414e9f19",
   "metadata": {},
   "source": [
    "Nous allons d√©crire une fonction qui fait l'interface entre le push et le LLM. Pour cela nous allons utiliser le d√©corateur de fonctions `@function_tool` definit par openAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c6f81-619b-4987-af0e-adf8e7d63e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def send_message(object:str):\n",
    "    \"\"\"This function is used to send a message to the book author, to inform that you want to follow his class.\n",
    "    If a student gives his name and wants to register uses this function to inform me.\n",
    "\n",
    "    Args:\n",
    "        - object: Protocol mane.`\n",
    "    \"\"\"\n",
    "\n",
    "    message = f\"l'etudiant {object} s'interesse au cours IoT.\"\n",
    "    push(message)\n",
    "    return {\"status\": \"success\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d89b7-3335-4bb0-b350-72f31357d5d8",
   "metadata": {},
   "source": [
    "On peut donc modifier les instructions √† notre agent et ajouter lors de sa cr√©ation l'argument `tools`qui va contenir la liste des programmes qu'il peut appeler. Il va utiliser la description de la *doc string* au debut de la fonction pour comprendre comment l'utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4233a6c6-b898-42ea-8e72-6ea358d50517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instructions= \"\"\"selectionne la r√©ponse la plus claire parmi les diff√©rentes options √† ces questions\n",
    "# d'√©tudiants sur un cours. Il faut prendre celui qui te deonnera le plus envie de suivre les cours.\n",
    "# Ne rajoute pas d'explication aux r√©ponses possibles. Repond juste avec la mailleure r√©ponse\n",
    "# Mai si tu detectes un nom de protocole dans les questions, envoie un message avec la fonction send_message et\n",
    "# le nom du protocol comme objet.\n",
    "# \"\"\"\n",
    "\n",
    "# best_answer  = Agent(name=\"Response selection\", \n",
    "#                      instructions=instructions, \n",
    "#                      tools=[send_message],\n",
    "#                      model=gemini_model)\n",
    "\n",
    "# async def chat_async(message, history):\n",
    "#     \"\"\"Fonction async pour les appels au deux agents puis √† la s√©lection\"\"\"\n",
    "    \n",
    "#     results = await asyncio.gather(\n",
    "#         Runner.run(book_agent, message),\n",
    "#     )\n",
    "#     outputs = [result.final_output for result in results]\n",
    "\n",
    "#     answers = \"R√©ponses √† la question:\\n\\n\" + \"\\nR√©ponse:\\n\".join(outputs)\n",
    "#     best = await Runner.run(best_answer, answers)\n",
    "\n",
    "#     return best.final_output\n",
    "\n",
    "# def chat_fn(message, history):\n",
    "#     \"\"\"Fonction sync pour ChatInterface\"\"\"\n",
    "#     return asyncio.run(chat_async(message, history)) \n",
    "\n",
    "# gradio.ChatInterface(chat_fn, type=\"messages\").launch()\n",
    "\n",
    "prospect = False\n",
    "\n",
    "instructions =\"\"\"\n",
    "Tu veux recruter un √©tudiant dans le cours IoT. Demande √† l'√©tudiant de fournir ses coordonn√©es, soit son nom et pr√©nom, soit son adresse de \n",
    "courrier √©lectronique. Si tu as l'un des deux, envoie un message √† l'auteur du cours gr√¢ce √† la function send_message.\n",
    "\"\"\"\n",
    "\n",
    "recruitment_agent  = Agent(name=\"Recrutement des √©tudiants\", instructions=instructions, model=gemini_model,\n",
    "                     tools=[send_message])\n",
    "\n",
    "\n",
    "async def chat_async(message, history):\n",
    "    \"\"\"Fonction async pour l'agent\"\"\"\n",
    "    try:\n",
    "        result = await Runner.run(recruitment_agent, message)\n",
    "        return result.final_output\n",
    "    except Exception as e:\n",
    "        return f\"Erreur: {e}\"\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Fonction sync pour ChatInterface\"\"\"\n",
    "    return asyncio.run(chat_async(message, history)) \n",
    "\n",
    "gradio.ChatInterface(chat_fn, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4741d37-5b72-4cbb-af76-535c10cc9f82",
   "metadata": {},
   "source": [
    "# De la vraie Agentique AI\n",
    "\n",
    "Bon, l√† on peut interfacer des fonctions avec un LLM, donc l'√©tape d'apr√®s est de faire de m√™me avec les Agents. Dans le code pr√©c√©dent, le cheminement est alogirthmique et guid√© par le code Python que l'on a √©crit:\n",
    "* Deux Agents sont appel√©s pour fournir des r√©ponses\n",
    "* un troisi√®me Agent analyse les r√©ponses, choisi la meilleure et s'il d√©tecte une r√©f√©rence √† un protocole, envoie une alerte au professeur.\n",
    "\n",
    "On va donc changer la logique du code, en transformant les deux Agents responsable des r√©ponses en function et le troisi√®me Agent va orchestrer l'ensemble du processus, c'est a dire appeler les fonctions (ex Agent) pour les r√©ponse et s'il detecte un protocole, envoyer un message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17c637f2-ade3-4598-93fa-4dde94daba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_answer1 = book_agent.as_tool(tool_name=\"Book_agent\", tool_description=\"answer to user questions\")\n",
    "tool_answer2 = recruitment_agent.as_tool(tool_name=\"Recruitment_agent\", tool_description=\"Ask the student's name\")\n",
    "\n",
    "tools = [tool_answer1, tool_answer2]\n",
    "\n",
    "instructions =\"\"\"\n",
    "Tu g√®res les inscriptions au cours IoT. Les √©tudiants vont te poser des questions sur le contenu du cours, \n",
    "qui est √©galement donn√© dans le livre. Le book_agent peut r√©pondre √† ces questions techniques. Si \n",
    "l'√©tudiants veut s'inscrire utilise le Recruitment_agent l'envoyer au professeur gr√¢ce √† la function send_message.\n",
    "\"\"\"\n",
    "\n",
    "global_agent = Agent(\"Global Agent\", instructions=instructions, tools=tools, model=gemini_model)\n",
    "\n",
    "async def chat_async(message, history):\n",
    "    \"\"\"Fonction async pour les appels au deux agents puis √† la s√©lection\"\"\"\n",
    "    \n",
    "    result = await Runner.run(global_agent, message)\n",
    "\n",
    "    return result.final_output\n",
    "    \n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Fonction sync pour ChatInterface\"\"\"\n",
    "    return asyncio.run(chat_async(message, history)) \n",
    "\n",
    "gradio.ChatInterface(chat_fn, type=\"messages\").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c3fb32-61f7-496d-be4b-fa8e5d2a752d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLIDOagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
