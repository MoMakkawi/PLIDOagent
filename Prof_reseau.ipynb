{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b198ef-1cce-4e75-a49b-a129fb20f8f6",
   "metadata": {},
   "source": [
    "<img src=\"images/network_prof.png\" width=\"150\" alt=\"Prof looking at a electronic brain\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\"> Dans cette partie, nous allons créer de vrais agents pour répondre aux questions des élèves sur le contenu du cours réseau. \n",
    "\n",
    "Avec le professeur de philosophie, nous avons vu les interrogrations de base d'un LLM, avec des appels de bas niveau.\n",
    "Cela demandait pas mal de code, et surtout les interrogrations étaient séquentielles. Quand nous avons demander à plusieurs LLM de plancer sur le devoir, il a fallu attendre la réponse\n",
    "de la première pour lancer la seconde, alors qu'elles auraient faire ce travail en même temps.\n",
    "\n",
    "Voici la liste des modules dont nous aurons besoin dans cette partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c890d334-bf11-46ce-ad5c-c3445ccdbbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "from IPython.display import Markdown, display \n",
    "\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from agents import Agent, Runner, trace, function_tool, OpenAIChatCompletionsModel, input_guardrail, GuardrailFunctionOutput\n",
    "\n",
    "import gradio\n",
    "import asyncio\n",
    "import requests \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0c84e-acd3-424b-ba57-6e8a8ab0e831",
   "metadata": {},
   "source": [
    "## Retrouver le contenu du livre \"Programmer l'Internet des Objets\"\n",
    "\n",
    "Dans un premier temps, nous allons convertir en texte les premières pages du livre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12884f1a-3e0f-4431-92ee-5a9c36b189aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book = PdfReader(\"./PLIDO_BOOK_en.pdf\")\n",
    "book_content = \"\"\n",
    "for page in book.pages:\n",
    "    text = page.extract_text()\n",
    "    book_content += text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d8d52-7ddc-4572-8e7f-3e382c75119d",
   "metadata": {},
   "source": [
    "## Donner l'information au LLM\n",
    "\n",
    "<img src=\"images/agent.png\" width=\"150\" alt=\"One agent\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\">Une fois les clés API chargées pour plusieurs serveurs (on utilisera par défaut celui de l'Université de Rennes, mais l'utilisation de Gemini est aussi possible). On donne l'URI et le Token pour le service, puis dans un deuxième temps, on précise le modèle LLM utilisé. Si on utilisait par defaut OpenAI, ces lignes seraient inutiles. Lors de l'appel d'```Agent``` , dans le cas d'OpenAI, le nom du modèle est directement indiqué dans une chaîne de caractères. \n",
    "\n",
    "A la création de l'Agent, lui donne un identifiant pour les traces, puis les instructions qui vont lui indiquer son rôle, les limites des réponses et les actions qu'il devra faire dans certain cas. Ces instructions contiennent également l'integralité du livre en ASCII. A noter que l'on demande au LLM de ne pas chercher à répondre en détail si la réponse ne se trouve pas dans le livre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3baea2-a9a5-47a9-94bb-e4b5fcd978c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "rennes_api_key = os.getenv(\"RENNES_API_KEY\")\n",
    "if not rennes_api_key:\n",
    "    print(\"RENNES_API_KEY is missing\")\n",
    "    exit(1)\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not rennes_api_key:\n",
    "    print(\"GOOLE_API_KEY is missing\")\n",
    "    exit(1)\n",
    "    \n",
    "RENNES_BASE_URL = \"https://ragarenn.eskemm-numerique.fr/sso/ch@t/api\"\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "rennes_client = AsyncOpenAI(base_url=RENNES_BASE_URL, api_key=rennes_api_key)\n",
    "rennes_model  = OpenAIChatCompletionsModel(model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", openai_client=rennes_client)\n",
    "\n",
    "gemini_client = AsyncOpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "gemini_model  = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=gemini_client)\n",
    "\n",
    "ollama_client = AsyncOpenAI(base_url=OLLAMA_BASE_URL, api_key=\"dont matter\")\n",
    "ollama_model  = OpenAIChatCompletionsModel(model=\"mistral:latest\", openai_client=ollama_client)\n",
    "\n",
    "\n",
    "instructions = f\"\"\"Voici le contenu d'un livre sur l'Internet des Objets\n",
    "\n",
    "{book_content}\n",
    "\n",
    "Ta responsabilité est de représenter l'auteur (Laurent Toutain) pour des interactions avec les élèves.\n",
    "Les réponses doivent être professionnelles, claires, et doivent donner envie aux étudiants de s'engager\n",
    "dans le cours, voire de choisir cette formation.\n",
    "Si tu ne connais pas la réponse aux questions, répond Non.\"\"\"\n",
    "\n",
    "book_agent = Agent(name=\"PLIDO Book Agent\", instructions=instructions, model=gemini_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7e563-0477-4dd9-8e7b-d8f8faddfda5",
   "metadata": {},
   "source": [
    "Un agent se lance en utilisant la coroutine Python ```run``` du module ```Runner``` importé du module ```agents``` d'OpenAI.  L'utilisation du mot-clé ```await``` est indispensable. Ici, la différence avec une fonction est minime, vu que l'on ne lance qu'une coroutine et que l'on attend sa fin pour passer à l'instruction suivante. \n",
    "\n",
    "Vous pouvez relancer la cellule suivante plusieurs fois en changeant la question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3883e9ec-d187-4389-8e5b-032482789221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "En tant qu'assistant de Laurent Toutain, je dirais que cela dépend de votre définition d'une bonne lecture de plage ! \n",
       "\n",
       "Si vous cherchez une lecture légère et divertissante pour vous détendre complètement, ce livre est peut-être un peu dense pour la plage.\n",
       "\n",
       "En revanche, si vous aimez utiliser votre temps à la plage pour apprendre et approfondir vos connaissances, et que le sujet de l'Internet des Objets vous passionne, alors ce livre peut être une lecture enrichissante, même à la plage. Imaginez-vous en train de décortiquer les protocoles IoT tout en profitant du soleil et du bruit des vagues !\n",
       "\n",
       "Dans tous les cas, n'oubliez pas votre crème solaire et vos lunettes de soleil !\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await Runner.run(book_agent, \"Est ce que le livre est un bon livre à lire à la plage?\")\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "487fe2be-936b-40a6-acfe-3dc899c14f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7882\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7882/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def chat_async(message, history):\n",
    "    \"\"\"Fonction async pour l'agent\"\"\"\n",
    "    try:\n",
    "        result = await Runner.run(book_agent, message)\n",
    "        return result.final_output\n",
    "    except Exception as e:\n",
    "        return f\"Erreur: {e}\"\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Fonction sync pour ChatInterface\"\"\"\n",
    "    return asyncio.run(chat_async(message, history)) \n",
    "\n",
    "gradio.ChatInterface(chat_fn, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8179ef0-93d9-4a54-b2cd-39e9e349d92a",
   "metadata": {},
   "source": [
    "On peut voir que cela amène à quelques acrobaties dans Python. Gradio appelle une fonction en callback pour traiter la commande de l'utilisateur, et l'interaction avec l'agent se fait par une coroutine. D'où la fonction `chat_fn` qui ne fait qu'appeler la coroutine, attend la fin de son execution grâce à `asyncio.run` et retourne le résultat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042cd5f-110b-4520-84b5-96afb4f5359a",
   "metadata": {},
   "source": [
    "# Plusieurs agents \n",
    "\n",
    "<img src=\"images/agents.png\" width=\"150\" alt=\"Several agents\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\"> Nous allons reprendre une structure calssique pour voir comment exploiter le parallisme avec `asyncio`, nous allons demander à deux LLM de cogiter sur la même question et l'on prendra la meilleure des deux. Comme nous allons utiliser le modèle ollama en local, il y a peu de chance qu'elle fasse la réponse la plus futée, mais sait-on jamais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08102273-53f0-414d-9d1d-a0499784119d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7886\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7886/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_agent = Agent(name=\"PLIDO Book Agent by Ollama\", instructions=instructions, model=ollama_model)\n",
    "\n",
    "instructions= \"\"\"selectionne la réponse la plus claire parmi les différentes options à ces questions\n",
    "d'étudiants sur un cours. Il faut prendre celui qui te deonnera le plus envie de suivre les cours.\n",
    "Ne rajoute pas d'explication aux réponses possibles. Repond juste avec la mailleure réponse\"\"\"\n",
    "\n",
    "best_answer  = Agent(name=\"Response selection\", instructions=instructions, model=gemini_model)\n",
    "\n",
    "async def chat_async(message, history):\n",
    "    \"\"\"Fonction async pour les appels au deux agents puis à la sélection\"\"\"\n",
    "    \n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(book_agent, message),\n",
    "        Runner.run(ollama_agent, message),\n",
    "    )\n",
    "    outputs = [result.final_output for result in results]\n",
    "\n",
    "    answers = \"Réponses à la question:\\n\\n\" + \"\\nRéponse:\\n\".join(outputs)\n",
    "    best = await Runner.run(best_answer, answers)\n",
    "\n",
    "    return best.final_output\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Fonction sync pour ChatInterface\"\"\"\n",
    "    return asyncio.run(chat_async(message, history)) \n",
    "\n",
    "gradio.ChatInterface(chat_fn, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60809034-330a-41d2-904b-c91ca3c0d1d5",
   "metadata": {},
   "source": [
    "# Envoi de message\n",
    "<img src=\"images/telephone.png\" width=\"150\" alt=\"Several agents\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\">\n",
    "Nous pouvons demander au LLM d'interagir avec le monde extérieur. pour cela nous allons utiliser une application qui envoie des messages sur votre téléphone portable.\n",
    "\n",
    "* téléchargez depuis votre magasin d'application (Android ou Apple) l'application `pushover`\n",
    "* Créez votre compte\n",
    "* Loguez vous également depuis votre ordinateur\n",
    "* Recupérez votre clé d'utilisateur qui apparait en haut à droite et stockez là dans la la variable `PUSHOVER_USER_ID` dans le fichier `.env`\n",
    "* En bas de la page, choisissez *Your Applications* et cliquez sur *Create an Application/Token*\n",
    "  * Donnez un nom comme *PLIDOagent*, et une fois validé, un token va apparaître\n",
    "  * Mettez ce token dans la variable `PUSHOVER_TOKEN` dans le fichier `.env`\n",
    "\n",
    "Le petit programme ci dessus vous permet de teste si ca marche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a75c7f9-d628-4951-9b18-32690ea0cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushover_token=os.getenv(\"PUSHOVER_TOKEN\")\n",
    "pushover_user_id=os.getenv(\"PUSHOVER_USER_ID\")\n",
    "pushover_uri =\"https://api.pushover.net/1/messages.json\"\n",
    "\n",
    "if not pushover_token or not pushover_user_id:\n",
    "    print (\"User_id or token missing\")\n",
    "    exit(1)\n",
    "\n",
    "def push(message):\n",
    "    payload = {\"user\": pushover_user_id, \"token\": pushover_token, \"message\": message}\n",
    "    x = requests.post(pushover_uri, data=payload)\n",
    "\n",
    "push(\"Hello my dear\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99f4dd-e4ad-439c-b44e-d92f414e9f19",
   "metadata": {},
   "source": [
    "Nous allons décrire une fonction qui fait l'interface entre le push et le LLM. Pour cela nous allons utiliser le décorateur de fonctions `@function_tool` definit par openAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "770c6f81-619b-4987-af0e-adf8e7d63e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def send_message(object:str):\n",
    "    \"\"\"This function is used to send a message to the book author, to inform that you want to follow his class.\n",
    "    If a student gives his name and wants to register uses this function to inform me.\n",
    "\n",
    "    Args:\n",
    "        - object: Protocol mane.`\n",
    "    \"\"\"\n",
    "\n",
    "    message = f\"Un etudiant s'interesse à  {object}.\"\n",
    "    push(message)\n",
    "    return {\"status\": \"success\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d89b7-3335-4bb0-b350-72f31357d5d8",
   "metadata": {},
   "source": [
    "On peut donc modifier les instructions à notre agent et ajouter lors de sa création l'argument `tools`qui va contenir la liste des programmes qu'il peut appeler. Il va utiliser la description de la *doc string* au debut de la fonction pour comprendre comment l'utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4233a6c6-b898-42ea-8e72-6ea358d50517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7892\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7892/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions= \"\"\"selectionne la réponse la plus claire parmi les différentes options à ces questions\n",
    "d'étudiants sur un cours. Il faut prendre celui qui te deonnera le plus envie de suivre les cours.\n",
    "Ne rajoute pas d'explication aux réponses possibles. Repond juste avec la mailleure réponse\n",
    "Maisi tu detectes un nom de protocole dans les réponses envoie un message avec la fonction send_message et\n",
    "le nom du protocol comme objet.\n",
    "\"\"\"\n",
    "\n",
    "best_answer  = Agent(name=\"Response selection\", \n",
    "                     instructions=instructions, \n",
    "                     tools=[send_message],\n",
    "                     model=gemini_model)\n",
    "\n",
    "async def chat_async(message, history):\n",
    "    \"\"\"Fonction async pour les appels au deux agents puis à la sélection\"\"\"\n",
    "    \n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(book_agent, message),\n",
    "        Runner.run(ollama_agent, message),\n",
    "    )\n",
    "    outputs = [result.final_output for result in results]\n",
    "\n",
    "    answers = \"Réponses à la question:\\n\\n\" + \"\\nRéponse:\\n\".join(outputs)\n",
    "    best = await Runner.run(best_answer, answers)\n",
    "\n",
    "    return best.final_output\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Fonction sync pour ChatInterface\"\"\"\n",
    "    return asyncio.run(chat_async(message, history)) \n",
    "\n",
    "gradio.ChatInterface(chat_fn, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4741d37-5b72-4cbb-af76-535c10cc9f82",
   "metadata": {},
   "source": [
    "# De la vraie Agentique AI\n",
    "\n",
    "Bon, là on peut interfacer des fonctions avec un LLM, donc l'étape d'après est de faire de même avec les Agents. Dans le code précédent, le cheminement est alogirthmique et guidé par le code Python que l'on a écrit:\n",
    "* Deux Agents sont appelés pour fournir des réponses\n",
    "* un troisième Agent analyse les réponses, choisi la meilleure et s'il détecte une référence à un protocole, envoie une alerte au professeur.\n",
    "\n",
    "On va donc changer la logique du code, en transformant les deux Agents responsable des réponses en function et le troisième Agent va orchestrer l'ensemble du processus, c'est a dire appeler les fonctions (ex Agent) pour les réponse et s'il detecte un protocole, envoyer un message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c637f2-ade3-4598-93fa-4dde94daba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7908\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7908/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_answer1 = book_agent.as_tool(tool_name=\"Book_agt_answer\", tool_description=\"answer to user questions\")\n",
    "tool_answer2 = ollama_agent.as_tool(tool_name=\"Ollama_agt_answer\", tool_description=\"answer to user questions\")\n",
    "\n",
    "tools = [tool_answer1, tool_answer2, send_message]\n",
    "\n",
    "instructions =\"\"\"\n",
    "Tu gères les réponses aux questions d'étudiant sur un livre et un cours réseau. Ton but est de selectionner la meilleure réponse\n",
    "faite par des Agents qui ont la connaissance du contenu du cours et du livre. \n",
    "\n",
    "Si tu detecte dans la question ou la reponse que l'étudiant demande des informations concernant un protocole réseau précis, \n",
    "envoie un message à l'auteur du livre du cours  avec la fonction send_message et le nom du protocole, donne la réponse complète à \n",
    "l'étudiant et informe le que sa requete a été enregistrée a des fins de statistiques.\n",
    "\n",
    "1: demande aux agents de répondre au question\n",
    "2: selectionne la meilleure réponse dans le sens où la réponse contient des détails sur le cours et que le ton est courtois et engageant\n",
    "3: si l'étudiant demande des informations concernant un protocole réseau précis, envoie un message au professeur responsable du cours \n",
    "avec la fonction send_message.\n",
    "\n",
    "Règles importantes:\n",
    "- tu dois imperativement utiliser les agents répondeurs pour faire les brouillons de réponse, ne le fait pas toi même.\n",
    "- Retourne à l'étudiant, une seule des réponses proposées.\n",
    "\"\"\"\n",
    "\n",
    "global_agent = Agent(\"Global Agent\", instructions=instructions, tools=tools, model=gemini_model)\n",
    "\n",
    "async def chat_async(message, history):\n",
    "    \"\"\"Fonction async pour les appels au deux agents puis à la sélection\"\"\"\n",
    "    \n",
    "    result = await Runner.run(global_agent, message)\n",
    "\n",
    "    return result.final_output\n",
    "    \n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Fonction sync pour ChatInterface\"\"\"\n",
    "    return asyncio.run(chat_async(message, history)) \n",
    "\n",
    "gradio.ChatInterface(chat_fn, type=\"messages\").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c3fb32-61f7-496d-be4b-fa8e5d2a752d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
