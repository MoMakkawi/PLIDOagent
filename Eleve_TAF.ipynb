{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2529fdd5",
   "metadata": {},
   "source": [
    "# MCP\n",
    "\n",
    "<img src=\"images/student.png\" width=\"150\" alt=\"Several agents\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\"> Nous allons maintenant écrire une série d'agents pour conseiller les étudiants de l'IMT Atlantique dans leur chox de TAF. Nous allons définir des agents qui vont collecter des données sur les TAF disponibles, et dans un second temps répondre aux questions des étudiants pour positionner favorablement la TAF IoT vis à vis des autres.\n",
    "\n",
    "Pour cela nous allons avoir besoin d'avoir des agents capables d'interagir avec l'extérieur pour aller chercher des informations sur le Web, ou même de stocker des informations pour assurer une cohérence entre deux interrogations. Nous avons vu pécédemment que l'on pouvait definir des outils, soit en utilisant le decorateur `@tool` qui va reprendre la fonction et utiliser la docstring pour comprendre le fonctionnement de la fonction et les arguments à lui fournir. \n",
    "\n",
    "Il est également possible dans l'API d'OpenAI de transformer un agent en outils avec la méthode `as_tool`.\n",
    "\n",
    "Mais ici, nous allons utiliser des fonctions déjà packagées. La représentation de ces fonctions grâce à MCP (Model Context Protocol) est très populaire. Elle a été definie par Anthropic, la société qui fournit Claude. Il ne s'agit pas vraiment d'un protocole au sens réseaux, mais d'un habillage d'outils pour permettre une integration rapide dans un Agent.\n",
    "\n",
    "MCP se decompose en deux élement un client et un serveur qui généralement tournent sur la même machine et utilisent les entrée/sorties standards (stdio) pour communiquer. Donc dans le code de l'agent, il faut:\n",
    "* installer le serveur MCP\n",
    "* lancer le serveur dans une co-routine\n",
    "* donner à l'agent une description des outils et des formats utilisés par l'agent.\n",
    "\n",
    "Un agent peut utiliser plusieurs serveur MCP, mais commençons par un exemple simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe830a",
   "metadata": {},
   "source": [
    "# Contexte\n",
    "\n",
    "<img src=\"images/brain.png\" width=\"150\" alt=\"Several agents\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\"> Nous avions soulevé le problème la dernière fois avec la fenêtre de dialogue. Comme chaque requête est exécutée independamment de l'autre, le LLM ne se rappelle pas des informations que l'utilisateur lui a fournies. Une solution était de lui fournir l'historique du dialogue, mais les données ne sont pas structurées. \n",
    "\n",
    "\n",
    "Une solution consiste à enregistrer les relations entre élements apprises pendant le dialogue. Il existe un outil  `mcp-memory-libsql`. Comme son nom l'indique, l'outil est appelable par MCP, il va mémoriser des informations en utilisant une base de données SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd2c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace, OpenAIChatCompletionsModel\n",
    "from openai import AsyncOpenAI\n",
    "from agents.mcp import MCPServerStdio\n",
    "\n",
    "from contextlib import AsyncExitStack\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Permet d'exécuter des boucles asynchrones\n",
    "\n",
    "load_dotenv()  # Charger les variables d'environnement depuis le fichier .env\n",
    "\n",
    "openai_model = OpenAIChatCompletionsModel(model=\"gpt-4.1-mini\", openai_client=AsyncOpenAI())\n",
    "\n",
    "RENNES_BASE_URL = \"https://ragarenn.eskemm-numerique.fr/sso/ch@t/api\"\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "# l'utilisation de os.environ permet de ne pas mettre les clés dans le code\n",
    "# mais de les stocker dans un fichier .env qui n'est pas partagé et de generer une erreur si la clé n'est pas trouvée\n",
    "\n",
    "rennes_client = AsyncOpenAI(base_url=RENNES_BASE_URL, api_key=os.environ[\"RENNES_API_KEY\"])\n",
    "rennes_model  = OpenAIChatCompletionsModel(model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", openai_client=rennes_client)\n",
    "\n",
    "gemini_client = AsyncOpenAI(base_url=GEMINI_BASE_URL, api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "gemini_model  = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=gemini_client)\n",
    "\n",
    "ollama_client = AsyncOpenAI(base_url=OLLAMA_BASE_URL, api_key=\"dont matter\")\n",
    "ollama_model  = OpenAIChatCompletionsModel(model=\"gemma3:1b\", openai_client=ollama_client)\n",
    "\n",
    "\n",
    "memory_path = os.path.abspath(os.path.join(os.getcwd(), \"memory/relations.db\"))\n",
    "\n",
    "MCP_params = [\n",
    "        {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"],\"env\":{\"MEMORY_FILE_PATH\": memory_path}},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bcaec4",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "file is not a database",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDatabaseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m cursor = conn.cursor()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Récupérer toutes les tables et leurs données\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT name FROM sqlite_master WHERE type=\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtable\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m tables = {table[\u001b[32m0\u001b[39m]: [\u001b[38;5;28mdict\u001b[39m(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m cursor.execute(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSELECT * FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)] \n\u001b[32m     18\u001b[39m           \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m cursor.fetchall()}\n\u001b[32m     20\u001b[39m conn.close()\n",
      "\u001b[31mDatabaseError\u001b[39m: file is not a database"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Chemin vers la DB MCP\n",
    "db_path = Path(\"/tmp/relations.db\")\n",
    "if not db_path.exists():\n",
    "    print(\"Base de données MCP non trouvée\")\n",
    "else:\n",
    "    # Dumper la DB\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Récupérer toutes les tables et leurs données\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = {table[0]: [dict(row) for row in cursor.execute(f\"SELECT * FROM {table[0]}\")] \n",
    "              for table in cursor.fetchall()}\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    # Afficher le résultat\n",
    "    print(f\"DB: {db_path}\")\n",
    "    for table, data in tables.items():\n",
    "        print(f\"{table}: {len(data)} lignes\")\n",
    "    \n",
    "    # Les données sont dans la variable 'tables'\n",
    "    print(json.dumps(tables, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0ad83",
   "metadata": {},
   "source": [
    "Les premières lignes n'ont plus de secrets pour vous.\n",
    "\n",
    "On crée plusieurs models vers plusieurs serveurs, vous pourrez choisir celui que vous préférez. Dans la suite, on va prendre OpenAI pour pouvoir tracer les requêtes, mais si vous êtes radin vous pouvez utilise Gemini, celui de Rennes ou même ollama à la place. Il faudra changer la variable dans le code quand le modèle sera défini.\n",
    "\n",
    "\n",
    "Pour invoquer un serveur MCP, on doit definir une strucutre indiquant:\n",
    "* comment installer le packetage:\n",
    "    * `uvx` qui va retrouver les package python comme on l'a fait manuellement jusqu'ici,\n",
    "    * `npx` qui va s'interesser aux packages javascript. Il faut installer `node`sur votre ordinateur dans une version supérieure à 18.\n",
    "* les arguments durant l'installation, ici:\n",
    "    * `-y` pour répondre oui à toutes les questions que l'installateur va poser, on a donc les choix par defaults\n",
    "    *  `mcp-memory-libsql`que l'installateur va trouver ici https://www.npmjs.com/package/@modelcontextprotocol/server-memory\n",
    "* les variables d'environnement:\n",
    "    * ici le chemin où la base SQL va être stockée\n",
    "    * mais on pourrait également trouver des Token d'API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0bad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_async(message, history):\n",
    "    \"\"\"Fonction async pour l'agent\"\"\"\n",
    "\n",
    "    # with avec AsyncExitStack permet de fermer automatiquement les ressources quand on sort du bloc\n",
    "    async with AsyncExitStack() as stack:\n",
    "        # On prend le tableau d'arguments pour chaque serveur MCP et on crée une instance de MCPServerStdio pour chacun\n",
    "        # le tableau mcp_servers contiendra les instances de MCPServerStdio\n",
    "        # Ce code est très générique et permet d'ajouter ou de retirer des serveurs MCP en modifiant uniquement le tableau MCP_params\n",
    "\n",
    "        mcp_servers = [await stack.enter_async_context(MCPServerStdio(params)) for params in MCP_params]\n",
    "\n",
    "        # ici one fait qu'afficher les outils disponibles et la description fournie au modèle.\n",
    "        for server in mcp_servers:\n",
    "            tools = await server.list_tools()  \n",
    "            for tool in tools:\n",
    "                print(f\"Tool available: {tool.name} - {tool.description}\")  \n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "asyncio.run(chat_async(\"Hello\", None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98072942",
   "metadata": {},
   "source": [
    "On va définir les instructions système pour notre Agent, pour lui indiquer comment utiliser la base de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2c823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error invoking MCP tool create_entities: ENOENT: no such file or directory, open '/Users/laurent/.npm/_npx/15b07286cbcc3329/node_modules/@modelcontextprotocol/server-memory/dist/memory/relations.db'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/agents/mcp/util.py\", line 104, in invoke_mcp_tool\n",
      "    result = await server.call_tool(tool.name, json_data)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/agents/mcp/server.py\", line 155, in call_tool\n",
      "    return await self.session.call_tool(tool_name, arguments)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/mcp/client/session.py\", line 264, in call_tool\n",
      "    return await self.send_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/mcp/shared/session.py\", line 286, in send_request\n",
      "    raise McpError(response_or_error.error)\n",
      "mcp.shared.exceptions.McpError: ENOENT: no such file or directory, open '/Users/laurent/.npm/_npx/15b07286cbcc3329/node_modules/@modelcontextprotocol/server-memory/dist/memory/relations.db'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/blocks.py\", line 1729, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/utils.py\", line 871, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/chat_interface.py\", line 545, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/gradio/chat_interface.py\", line 917, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/usr/local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/ty/8wjj4_wx107f3v8psl_fdtxh0000gq/T/ipykernel_16604/1591449624.py\", line 53, in chat_fn\n",
      "    return asyncio.run(chat_async(message, history))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/ty/8wjj4_wx107f3v8psl_fdtxh0000gq/T/ipykernel_16604/1591449624.py\", line 44, in chat_async\n",
      "    result = await Runner.run(\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/agents/run.py\", line 199, in run\n",
      "    return await runner.run(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/agents/run.py\", line 417, in run\n",
      "    turn_result = await self._run_single_turn(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/agents/run.py\", line 919, in _run_single_turn\n",
      "    return await cls._get_single_step_result_from_response(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/agents/run.py\", line 959, in _get_single_step_result_from_response\n",
      "    return await RunImpl.execute_tools_and_side_effects(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/agents/_run_impl.py\", line 249, in execute_tools_and_side_effects\n",
      "    function_results, computer_results = await asyncio.gather(\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 316, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/agents/_run_impl.py\", line 589, in execute_function_tool_calls\n",
      "    results = await asyncio.gather(*tasks)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 316, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/agents/_run_impl.py\", line 577, in run_single_tool\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.12/site-packages/agents/_run_impl.py\", line 551, in run_single_tool\n",
      "    _, _, result = await asyncio.gather(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/agents/mcp/util.py\", line 107, in invoke_mcp_tool\n",
      "    raise AgentsException(f\"Error invoking MCP tool {tool.name}: {e}\") from e\n",
      "agents.exceptions.AgentsException: Error invoking MCP tool create_entities: ENOENT: no such file or directory, open '/Users/laurent/.npm/_npx/15b07286cbcc3329/node_modules/@modelcontextprotocol/server-memory/dist/memory/relations.db'\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "Tu es un assistant conversationnel avec une mémoire persistante parfaite.\n",
    "\n",
    "PROTOCOLE DE MÉMOIRE OBLIGATOIRE :\n",
    "1. TOUJOURS commencer par search_nodes pour récupérer les informations existantes\n",
    "2. Quand l'utilisateur se présente, créer IMMÉDIATEMENT une entité avec create_entities\n",
    "3. À CHAQUE nouveau fait important, l'ajouter avec create_entities ou mettre à jour\n",
    "4. Créer des relations pertinentes avec create_relations\n",
    "\n",
    "EXEMPLE D'UTILISATION DES OUTILS :\n",
    "- Utilisateur dit \"Je m'appelle Laurent\" → create_entities avec name=\"Laurent\", entityType=\"person\", observations=[\"nom: Laurent\"]\n",
    "- Plus d'infos → ajouter à l'entité existante\n",
    "- Toujours rechercher avant de répondre\n",
    "\n",
    "IMPORTANT : Utilise SYSTÉMATIQUEMENT tes outils de mémoire pour CHAQUE conversation.\n",
    "\"\"\"\n",
    "\n",
    "async def chat_async(message, history):\n",
    "    \"\"\"Fonction async pour l'agent\"\"\"\n",
    "\n",
    "    # with avec AsyncExitStack permet de fermer automatiquement les ressources quand on sort du bloc\n",
    "    async with AsyncExitStack() as stack:\n",
    "        # On prend le tableau d'arguments pour chaque serveur MCP et on crée une instance de MCPServerStdio pour chacun\n",
    "        # le tableau mcp_servers contiendra les instances de MCPServerStdio\n",
    "        # Ce code est très générique et permet d'ajouter ou de retirer des serveurs MCP en modifiant uniquement le tableau MCP_params\n",
    "\n",
    "        mcp_servers = [await stack.enter_async_context(MCPServerStdio(params)) for params in MCP_params]\n",
    "\n",
    "        # ici one fait qu'afficher les outils disponibles et la description fournie au modèle.\n",
    "        #for server in mcp_servers:\n",
    "        #    tools = await server.list_tools()  \n",
    "        #    for tool in tools:\n",
    "        #        print(f\"Tool available: {tool.name} - {tool.description}\")  \n",
    "\n",
    "\n",
    "        recruiting_agent = Agent(\n",
    "            name=\"Recruiting Agent\",\n",
    "            instructions=instructions,\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            mcp_servers=mcp_servers\n",
    "            )\n",
    "\n",
    "        with trace(\"TAF LIST\"):\n",
    "            result = await Runner.run(\n",
    "                recruiting_agent,\n",
    "                message\n",
    "            )\n",
    "\n",
    "        return result.final_output\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Fonction sync pour ChatInterface\"\"\"\n",
    "    return asyncio.run(chat_async(message, history)) \n",
    "\n",
    "gradio.ChatInterface(chat_fn, type=\"messages\").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a9b89-d182-44d3-bced-3c4ea0aa5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import AsyncExitStack\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace, OpenAIChatCompletionsModel\n",
    "from openai import AsyncOpenAI\n",
    "from agents.mcp import MCPServerStdio\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "import nest_asyncio # for running asyncio in jupyter\n",
    "\n",
    "nest_asyncio.apply() # necessary when asyncio is used into a jupyter notebook\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "RENNES_BASE_URL = \"https://ragarenn.eskemm-numerique.fr/sso/ch@t/api\"\n",
    "rennes_client = AsyncOpenAI(base_url=RENNES_BASE_URL, api_key=os.environ[\"RENNES_API_KEY\"])\n",
    "rennes_model  = OpenAIChatCompletionsModel(model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", openai_client=rennes_client)\n",
    "\n",
    "brave_env = {\"BRAVE_API_KEY\": os.environ[\"BRAVE_API_KEY\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d797d",
   "metadata": {},
   "source": [
    "Dans un premier temps nous allos utiliser deux serveurs MCP:\n",
    "* server-filesystem qui va pouvoir écrire dans un répertoire que nous devons spécfier. c'est ce qui est fait au début du script. On rajoute `sandbox` à la fin du répertoire.\n",
    "* brave search qui va interroger le moteur de recherche. Pour pourvoir y accéder il faudra une clé d'API que vous allez pouvoir trouver en créant un compte sur `https://brave.com/search/api/`\n",
    "\n",
    "On crée ensuite une liste de serveurs MCP. Il existe deux méthodes principale pour les installer:\n",
    "* uvx qui va retrouver les package python \n",
    "* npx qui va s'interesser aux packages javascript. Il faut installe `node`sur votre ordinateur dans une version supérieure à 18.\n",
    "\n",
    "Ensuite on passe les arguments pour l'installation pour ces deux langages. On peut noter l'utilisation de la clé ènv`qui va permettre de passer des paramètres supplémentaires, comme la clé d'API pour Brave Search.\n",
    "\n",
    "Les lignes de codes suivantes sont un peu plus absconces. On crée avec le `async with`une partie de code où plusiers co-routine vont pouvoir s'exécuter en parallèle:\n",
    "* dans un premier temps on lance tous nos serveurs MCP, c'est à dire ceux qui ont été décrits dans le tableau `MCP_params`.\n",
    "* le résultat est un tableau `mcp_servers`où tous les serveurs MCP sont décrits, \n",
    "* Ensuite on définit notre agent, et on l'execute en parallèle avec tous les serveurs MCP.\n",
    "* Quand on quitte le block du `with`, toutes les connexions avec les serveurs sont fermées grace à la terminaison de `ÀsyncExitStack()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ac29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Définition des paramètres des serveurs MCP\n",
    "    sandbox_path = os.path.abspath(os.path.join(os.getcwd(), \"sandbox\"))\n",
    "\n",
    "\n",
    "\n",
    "    MCP_params = [\n",
    "        {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", sandbox_path]},\n",
    "        {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"], \"env\": brave_env}\n",
    "\n",
    "    ]\n",
    "\n",
    "    async with AsyncExitStack() as stack:\n",
    "        mcp_servers = [await stack.enter_async_context(MCPServerStdio(params)) for params in MCP_params]\n",
    "\n",
    "\n",
    "        instructions = \"\"\"\n",
    "        You browse the internet to accomplish your instructions.\n",
    "        You are highly capable at browsing the internet independently to accomplish your task, \n",
    "        including accepting all cookies and clicking 'not now' as\n",
    "        appropriate to get to the content you need. If the site asks for authentication,\n",
    "        validate, the credentials are already recorded.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Utilisation des serveurs MCP dans le contexte de l'agent\n",
    "\n",
    "            taf_list_agent = Agent(\n",
    "                name=\"TAF List Agent\",\n",
    "                instructions=instructions,\n",
    "                model=\"gpt-4.1-mini\",\n",
    "                mcp_servers=mcp_servers\n",
    "            )\n",
    "\n",
    "            with trace(\"TAF LIST\"):\n",
    "                result = await Runner.run(\n",
    "                    taf_list_agent,\n",
    "                    f\"\"\"\n",
    "                    L'IMT Atlantique propose pour les étudiants de 2eme et 3eme année des parcours de formation\n",
    "                    appelés TAF (Thématique d'Approfondissement de Formation).\n",
    "                    1) Cherche sur le web avec Brave, les TAF proposées par l'IMT Atlantique.\n",
    "                    2) construit leur nom sous la forme TAF xxxx, fait en une liste classée par ordre alphabetique\n",
    "                    et stocke la dans le fichier TAF.md en y indiquant la date de mise à jour.\n",
    "                    N'efface pas les données du fichier, ajoute juste les nouvelles TAF trouvées.\n",
    "                    \"\"\"\n",
    "                )\n",
    "                print(result.final_output)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur: {e}\")\n",
    "\n",
    "        print (\"End of agent\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d283584",
   "metadata": {},
   "source": [
    "C'est bien le .md pour les IA, mais pour un programme python c'est plus compliqué à gérer. De plus on voit que la date mise dans le fichier est un peu fantaisiste. On va donc modifier les instructions pour forcer la réponse en JSON.\n",
    "\n",
    "On va apporter d'autres modification:\n",
    "* on va augmenter le nombre d'itération de l'agent à 50 car il n'est pas si simple de retrouver les TAF dans l'Internet.\n",
    "* On va stocker le résultat au format JSON, notez que l'on indique la date actuelle dans la requête car les LLM ont une notion toute particulière du temps.\n",
    "* On va créer un autre agent qui va être utiliser pour créer un descriptif plus détaillé des TAF que l'on a trouvé. Si le fichier `TAF.json`n'existe pas, alors notre agent principal va pouvoir appeler celui qui s'est déclaré pour reconstruire cette liste:\n",
    "    * On ajoute la description `handoff_description` pour résumer ce que fait `TAF list agent`.\n",
    "    * et dans la description de l'agent principal, on lui dit qu'il peut déléguer et on pointe sur la liste des agents secondaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Définition des paramètres des serveurs MCP\n",
    "    sandbox_path = os.path.abspath(os.path.join(os.getcwd(), \"sandbox\"))\n",
    "\n",
    "\n",
    "\n",
    "    MCP_params = [\n",
    "        {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", sandbox_path]},\n",
    "        {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"], \"env\": brave_env}\n",
    "\n",
    "    ]\n",
    "\n",
    "    async with AsyncExitStack() as stack:\n",
    "        mcp_servers = [await stack.enter_async_context(MCPServerStdio(params)) for params in MCP_params]\n",
    "\n",
    "\n",
    "        list_instructions = \"\"\"\n",
    "        You browse the internet to accomplish your instructions.\n",
    "        You are highly capable at browsing the internet independently to accomplish your task, \n",
    "        including accepting all cookies and clicking 'not now' as\n",
    "        appropriate to get to the content you need. If the site asks for authentication,\n",
    "        validate, the credentials are already recorded.\n",
    "        \"\"\"\n",
    "\n",
    "        taf_list_agent = Agent(\n",
    "            name=\"TAF List Agent\", \n",
    "            instructions=list_instructions, \n",
    "            model=\"gpt-4.1-mini\",\n",
    "            mcp_servers=mcp_servers,\n",
    "            handoff_description=\"Use this agent to create or update the file TAF.json with the list of TAFs \\\n",
    "                offered by IMT Atlantique.\"        \n",
    "            )\n",
    "    \n",
    "        try:\n",
    "            # Utilisation des serveurs MCP dans le contexte de l'agent\n",
    "\n",
    "            content_instructions=\"\"\"\n",
    "            You are an agent that browses the internet to find and retrieve information. You goal is to build a \n",
    "            precise description for a particular TAF, listed in the TAF.md file. If this file does not exist, or \n",
    "            its content is more than one day old, you have to call the TAF List Agent to create or update it.\n",
    "            \"\"\"\n",
    "            taf_content_agent = Agent(\n",
    "                name=\"TAF content Agent\", \n",
    "                instructions=content_instructions, \n",
    "                model=\"gpt-4.1-mini\",\n",
    "                mcp_servers=mcp_servers,\n",
    "                handoffs=[taf_list_agent]\n",
    "            )\n",
    "\n",
    "            with trace(\"TAF Content\"):\n",
    "\n",
    "                result = await Runner.run(\n",
    "                    taf_content_agent,\n",
    "                    f\"\"\" \n",
    "                    Aujourd'hui on est le {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}.\n",
    "                    L'IMT Atlantique propose pour les étudiants de 2eme et 3eme année des parcours de formation\n",
    "                    appelés TAF (Thématique d'Approfondissement de Formation).\n",
    "\n",
    "                    1) prend la liste des TAF contenu dans TAF.json, si le fichier n'existe pas ou est vieux d'un jour,\n",
    "                    utilise l'agent TAF List Agent pour le créer ou le mettre à jour avec la liste des TAF trouvé par le\n",
    "                    moteur de recherche.\n",
    "                    2) pour chaque TAF contenue dans ce fichier, regarde si un fichier de description existe dans le répertoire.\n",
    "                    3) si on n'a pas de description de la TAF, cherche sur internet pour avoir une description précise\n",
    "                    du contenu avec un sylabus les plus détaillé possible, des métiers visés et de l'état du marché de\n",
    "                    l'emploi dans ce domaine. Crée pour chaqu'un d'eux\n",
    "                    un fichier spécifique dans le format markdown qui portera le nom de la TAF.\n",
    "\n",
    "                    Limite ton nombre de requêtes à 5 par secondes.\n",
    "                    \"\"\",\n",
    "                    max_turns=50\n",
    "                )\n",
    "                print(result.final_output)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur: {e}\")\n",
    "\n",
    "        print (\"End of agent\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549084e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f519d99f",
   "metadata": {},
   "source": [
    "Bon maintenant, on a rempli notre répertoire avec les informations concernant les TAF, mettez dedans PLIDO_BOOK_en.pdf\n",
    "\n",
    "```cp PLIDO_BOOK_en.pdf sandbox/````\n",
    "\n",
    "en on va pouvoir lancer un agent conversationnel, pour convaincre les étudiants de venir dans la TAF IoT. On reprend le code avec l'interface gradio que l'on avait développé avant, et l'on change les instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08503793-970c-4b3b-b810-5662b8c330ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import Markdown, display \n",
    "\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from agents import Agent, Runner, trace, function_tool, OpenAIChatCompletionsModel, input_guardrail, GuardrailFunctionOutput\n",
    "\n",
    "import gradio\n",
    "import asyncio\n",
    "import requests \n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "rennes_api_key = os.getenv(\"RENNES_API_KEY\")\n",
    "if not rennes_api_key:\n",
    "    print(\"RENNES_API_KEY is missing\")\n",
    "    exit(1)\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not rennes_api_key:\n",
    "    print(\"GOOLE_API_KEY is missing\")\n",
    "    exit(1)\n",
    "    \n",
    "RENNES_BASE_URL = \"https://ragarenn.eskemm-numerique.fr/sso/ch@t/api\"\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "rennes_client = AsyncOpenAI(base_url=RENNES_BASE_URL, api_key=rennes_api_key)\n",
    "rennes_model  = OpenAIChatCompletionsModel(model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", openai_client=rennes_client)\n",
    "\n",
    "gemini_client = AsyncOpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "gemini_model  = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=gemini_client)\n",
    "\n",
    "ollama_client = AsyncOpenAI(base_url=OLLAMA_BASE_URL, api_key=\"dont matter\")\n",
    "ollama_model  = OpenAIChatCompletionsModel(model=\"gemma3:1b\", openai_client=ollama_client)\n",
    "\n",
    "recruit_instructions = \"\"\"\n",
    "You MUST use your tools systematically:\n",
    "- ALWAYS start by checking memory for existing student information\n",
    "- ALWAYS store new student information (name, interests, background) in memory\n",
    "- ALWAYS read TAF documentation files when discussing program details\n",
    "- ALWAYS search the web when you need current information\n",
    "\n",
    "You are a recruitment agent. Your job is to convice students to apply to the TAF IoT. You have to\n",
    "be polite, understand the questionning of the student, but you have to be persuasive and give them \n",
    "all the information they need to make a decision. You can use the document in the directory and\n",
    "query the web to have more details information regarding the student's questions.\n",
    "\n",
    "If the student asks for, you can provide a combination of two ATF he can do during the first \n",
    "or the second year. One of them has to be the TAF IoT\n",
    "\n",
    "The TAF IoT is located in Rennes, you can add extra information to prove that Rennes is a dynamic city \n",
    "with a strong focus on technology and innovation. You can mention the presence of numerous tech companies, \n",
    "startups, and research institutions in the area, as well as the city's commitment to fostering innovation \n",
    "and supporting the growth of the digital economy. It is also a good place to party and have new friends.\n",
    "\"\"\"\n",
    "\n",
    "recruit_agent = Agent(name=\"recruit_agent\", instructions=recruit_instructions, model=gemini_model)\n",
    "\n",
    "\n",
    "async def chat_async(message, history):\n",
    "    \"\"\"Fonction async pour l'agent\"\"\"\n",
    "\n",
    "    sandbox_path = os.path.abspath(os.path.join(os.getcwd(), \"sandbox\"))\n",
    "    \n",
    "\n",
    "    MCP_params = [\n",
    "        {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"],\"env\":{\"MEMORY_FILE_PATH\": \"/tmp/memory.json\"}},\n",
    "        {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", sandbox_path]},\n",
    "        {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"], \"env\": {\"BRAVE_API_KEY\": os.environ[\"BRAVE_API_KEY\"]}}\n",
    "    ]\n",
    "\n",
    "    async with AsyncExitStack() as stack:\n",
    "        mcp_servers = [await stack.enter_async_context(MCPServerStdio(params)) for params in MCP_params]\n",
    "\n",
    "        try:\n",
    "            result = await Runner.run(recruit_agent, message)\n",
    "            return result.final_output\n",
    "        except Exception as e:\n",
    "            return f\"Erreur: {e}\"\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Fonction sync pour ChatInterface\"\"\"\n",
    "    return asyncio.run(chat_async(message, history)) \n",
    "\n",
    "gradio.ChatInterface(chat_fn, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62badff-32ef-4f96-991f-32789cc86f5f",
   "metadata": {},
   "source": [
    "What we can see, it that it is impossible to have a dialog with the chat engine. There is no memory, so when you answer to a question he asked, the next round the agent has forgottent everything. \n",
    "\n",
    "One solution is to use a SQL database to memorize all the dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df11d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import Markdown, display \n",
    "\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from agents import Agent, Runner, trace, function_tool, OpenAIChatCompletionsModel, input_guardrail, GuardrailFunctionOutput\n",
    "\n",
    "import gradio\n",
    "import asyncio\n",
    "import requests \n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "rennes_api_key = os.getenv(\"RENNES_API_KEY\")\n",
    "if not rennes_api_key:\n",
    "    print(\"RENNES_API_KEY is missing\")\n",
    "    exit(1)\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not rennes_api_key:\n",
    "    print(\"GOOLE_API_KEY is missing\")\n",
    "    exit(1)\n",
    "    \n",
    "RENNES_BASE_URL = \"https://ragarenn.eskemm-numerique.fr/sso/ch@t/api\"\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "rennes_client = AsyncOpenAI(base_url=RENNES_BASE_URL, api_key=rennes_api_key)\n",
    "rennes_model  = OpenAIChatCompletionsModel(model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", openai_client=rennes_client)\n",
    "\n",
    "gemini_client = AsyncOpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "gemini_model  = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=gemini_client)\n",
    "\n",
    "ollama_client = AsyncOpenAI(base_url=OLLAMA_BASE_URL, api_key=\"dont matter\")\n",
    "ollama_model  = OpenAIChatCompletionsModel(model=\"gemma3:1b\", openai_client=ollama_client)\n",
    "\n",
    "recruit_instructions = \"\"\"\n",
    "You use your entity tools as a persistent memory to store and recall information about your conversations.\n",
    "IMPORTANT: At the beginning of each conversation, you MUST:\n",
    "1. First, use the memory tools to check if you have any previous information about this student\n",
    "2. Store new information about the student (name, interests, preferences) in persistent memory\n",
    "3. Read the TAF documentation files to have current information about the programs\n",
    "\n",
    "You are a recruitment agent for the TAF IoT program. Your job is to convince students to apply.\n",
    "\n",
    "WORKFLOW for each interaction:\n",
    "1. Store/retrieve student information in memory\n",
    "2. If student asks about TAF details, read the relevant markdown files in the directory\n",
    "3. If you need current market information or specific details, search the web\n",
    "4. Always save important student preferences and questions to memory for future reference\n",
    "\n",
    "The TAF IoT is located in Rennes - mention the city's tech ecosystem and student life when relevant.\n",
    "\"\"\"\n",
    "Alternative : Instructions plus directives\n",
    "Ou encore plus directif :\n",
    "pythonrecruit_instructions = \"\"\"\n",
    "You MUST use your tools systematically:\n",
    "- ALWAYS start by checking memory for existing student information\n",
    "- ALWAYS store new student information (name, interests, background) in memory\n",
    "- ALWAYS read TAF documentation files when discussing program details\n",
    "- ALWAYS search the web when you need current information\n",
    "\n",
    "You are a recruitment agent for the TAF IoT program...\n",
    "[reste de vos instructions]\n",
    "\"\"\"\n",
    "Modification du code pour debugging\n",
    "Ajoutez aussi cette ligne pour voir quels outils sont disponibles :\n",
    "pythonrecruit_agent = Agent(\n",
    "    name=\"recruit_agent\", \n",
    "    instructions=recruit_instructions, \n",
    "    model=gemini_model,\n",
    "    mcp_servers=mcp_servers,\n",
    "    max_turns=10  # Limitez pour éviter les boucles infinies pendant les tests\n",
    ")\n",
    "\n",
    "\n",
    "async def chat_async(message, history):\n",
    "    \"\"\"Fonction async pour l'agent\"\"\"\n",
    "\n",
    "    sandbox_path = os.path.abspath(os.path.join(os.getcwd(), \"sandbox\"))\n",
    "    \n",
    "\n",
    "    MCP_params = [\n",
    "        {\"command\": \"npx\",\"args\": [\"-y\", \"mcp-memory-libsql\"],\"env\": {\"LIBSQL_URL\": \"file:./memory/history.db\"}},\n",
    "        {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", sandbox_path]},\n",
    "        {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"], \"env\": brave_env}\n",
    "    ]\n",
    "\n",
    "    async with AsyncExitStack() as stack:\n",
    "        mcp_servers = [await stack.enter_async_context(MCPServerStdio(params)) for params in MCP_params]\n",
    "\n",
    "        recruit_agent = Agent(name=\"recruit_agent\", \n",
    "                          instructions=recruit_instructions, \n",
    "                          model=gemini_model,\n",
    "                          mcp_servers=mcp_servers)\n",
    "        \n",
    "        try:\n",
    "            result = await Runner.run(recruit_agent, message)\n",
    "            return result.final_output\n",
    "        except Exception as e:\n",
    "            return f\"Erreur: {e}\"\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Fonction sync pour ChatInterface\"\"\"\n",
    "    return asyncio.run(chat_async(message, history)) \n",
    "\n",
    "gradio.ChatInterface(chat_fn, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
