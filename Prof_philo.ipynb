{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d8e77e-2bd7-47ba-91b0-fa428f6c2936",
   "metadata": {},
   "source": [
    "# Jouons au professeur de Philosophie\n",
    "\n",
    "<img src=\"images/prof.png\" width=\"150\" alt=\"Prof looking at a electronic brain\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\">  Dans cette partie pour nous initier à l'Agentique AI, et voir ce que c'est, nous allons prendre le rôle d'un professeur de philosophie qui doit rédiger un sujet, il le donnera à ses étudiants. Il corrigera les réponses et classera les élèves. Dans notre cas, tous ces acteurs seront mis en oeuvre par une LLM.\n",
    "\n",
    "Nous utiliserons plusieurs LLM, la pluspart gratuite, mais pour quelques euro vous pourrez accéder à d'autres services, mais ce n'est pas essentiel pour ces exercices. Nous utiliserons:\n",
    "\n",
    "* Plusieurs LLM mis à notre disposition par l'université de Rennes 1\n",
    "* Le LLM de Google\n",
    "* Un modèle tournant sur votre propre machine\n",
    "* et si vous le souhaitez OpenAI et Antropic (payant: ~5€)\n",
    "\n",
    "# Mise en oeuvre\n",
    "\n",
    "## API\n",
    "\n",
    "Pour communiquer avec un LLM, nous allons utiliser une API REST, et pour pouvoir nous identifier, nous devons devoir créer un jeton (token), sur son site. Notre LLN de référence est hébergée à l'Universite de Rennes. Vous pouvez vous y connecter avec votre login IMT Atlantique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896d1be-a7a0-4888-9c96-02582f77f5ee",
   "metadata": {},
   "source": [
    "### LLMs de l'Université de Rennes \n",
    "\n",
    "Pour vous connecter aux machines de l'Université de Rennes, cliquez sur ce lien: https://ragarenn.eskemm-numerique.fr/sso/ch@t/app/auth\n",
    "\n",
    "* Selectionnez IMT Atlantique\n",
    "* Logguez vous\n",
    "\n",
    "<img src=\"images/UR-param.png\" width=\"100\" alt=\"Prof looking at a electronic brain\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\">  En haut, à droite, vous avez accès à votre environnement. Cliquez sur **Paramètres**; sur **Compte** et sur **Clés d'API**.\n",
    "\n",
    "* Créez votre clé d'API et copiez là.\n",
    "\n",
    "* Editez le fichier .env qui se trouve dans ce répertoire et ajoutez la ligne **RENNES_API_KEY==**<<clé copiée>>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ace219-9d5e-4b0b-be1d-48b14b180ce6",
   "metadata": {},
   "source": [
    "### Google Gemini \n",
    "\n",
    "Google permet d'accèder gratuitement à son LLM, nous devons également récupérer une clé d'API.\n",
    "\n",
    "* Allez sur le site https://aistudio.google.com/apikey (connectez vous à votre compte Google)\n",
    "* Creez une clé d'API et copiez là.\n",
    "* Ajouter une ligne dans le fichier .env **GOOGLE_AI_KEY==**<<clé copiée>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34497c18-5257-4226-9994-c3a6cf4a5ce7",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "\n",
    "Ollama permet de faire tourner localement un LLM sur votre machine. Bien sûr, ce sera moins performant que sur un serveur spécialisé, mais celà pourra sufir dans certains cas, ou celà permettra de comparer différents modèles.\n",
    "\n",
    "* téléchargez l'exécutable correspondant à votre système sur https://ollama.com/\n",
    "* Installez le programme\n",
    "\n",
    "Ollama fonctionne par ligne de commande, tapez depuis un shell:\n",
    "\n",
    "* `ollama pull mistral` pour installer un premier modèle\n",
    "* `ollama serve` pour lancer le serveur\n",
    "* `ollama run mistral`\n",
    "* Ouvrez un onlglet dans votre navigateur pour vérifier que ollama est actif http://localhost:11434. Le message 'Ollama is running' doit apparaitre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7296f4-d85b-48fb-8d7f-daf0324df37b",
   "metadata": {},
   "source": [
    "## Environnement de travail\n",
    "\n",
    "### uv\n",
    "\n",
    "uv est un gestionnaire de module Python, très simple et puissant. Pour l'installer:\n",
    "\n",
    "* tapez `curl -LsSf https://astral.sh/uv/install.sh | sh` (vous pouvez aller aussi sur ce site pour plus de détails https://docs.astral.sh/uv/getting-started/installation/)\n",
    "* `uv self update` pour vérifier que tout marche\n",
    "* `uv sync` pour charger les modules Python nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d31d67-52c9-4dba-b188-270079a857ee",
   "metadata": {},
   "source": [
    "# Première interrogation\n",
    "\n",
    "<img src=\"images/work.png\" width=\"150\" alt=\"Workers\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\"> \n",
    "\n",
    "Nous allons interroger le serveur de l'Université de Rennes pour avoir une réponse à une question. \n",
    "\n",
    "La première étape consiste à récupérer la clé d'API qui nous avons stockée dans le fichier `.env`. Nous allons utiliser le module `dotenv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb4a99c-b58a-4715-897e-59ced8e3712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour l'exécution de cette cellule, choisr dans Visual Studio le kernel PLIDOagent\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# read .env file to setup variables. override==True allows the erase old settings\n",
    "load_dotenv(override=True)\n",
    "rennes_api_key = os.getenv('RENNES_API_KEY')\n",
    "\n",
    "if not rennes_api_key:\n",
    "    print(\"Error, could not find the API key for University of Rennes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c63ddb-e512-4682-b6ce-b8d290d24bab",
   "metadata": {},
   "source": [
    "Nous allons interroger le serveur de l'Université de Rennes pour connaître les modèles disponibles. Depuis les navigaiteur que vous avez utilisé pour vous connecter au serveur de l'Université de Rennes, entrez cet URI:\n",
    "\n",
    "* https://ragarenn.eskemm-numerique.fr/sso/ch@t/api/models\n",
    "\n",
    "Le résultat, une fois reformaté ressemble à ceci:\n",
    "\n",
    "```\n",
    "{\n",
    "   \"data\":[\n",
    "      {\n",
    "         \"id\":\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\",\n",
    "         \"object\":\"model\",\n",
    "         \"created\":1753696990,\n",
    "         \"owned_by\":\"openai\",\n",
    "         \"root\":\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\",\n",
    "         \"parent\":null,\n",
    "         \"max_model_len\":128000,\n",
    "         \"permission\":[\n",
    "            {\n",
    "               \"id\":\"modelperm-c81ba25fc5bd4f8ea3be8f754fdff426\",\n",
    "               \"object\":\"model_permission\",\n",
    "               \"created\":1753696990,\n",
    "               \"allow_create_engine\":false,\n",
    "               \"allow_sampling\":true,\n",
    "               \"allow_logprobs\":true,\n",
    "               \"allow_search_indices\":false,\n",
    "               \"allow_view\":true,\n",
    "               \"allow_fine_tuning\":false,\n",
    "               \"organization\":\"*\",\n",
    "               \"group\":null,\n",
    "               \"is_blocking\":false\n",
    "            }\n",
    "         ],\n",
    "         \"name\":\"Mistral Small 3.1 24B\",\n",
    "         \"openai\":{\n",
    "            \"id\":\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\",\n",
    "            \"object\":\"model\",\n",
    "            \"created\":1753696990,\n",
    "            \"owned_by\":\"vllm\",\n",
    "            \"root\":\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\",\n",
    "            \"parent\":null,\n",
    "            \"max_model_len\":128000,\n",
    "            \"permission\":[\n",
    "               {\n",
    "                  \"id\":\"modelperm-c81ba25fc5bd4f8ea3be8f754fdff426\",\n",
    "                  \"object\":\"model_permission\",\n",
    "                  \"created\":1753696990,\n",
    "                  \"allow_create_engine\":false,\n",
    "                  \"allow_sampling\":true,\n",
    "                  \"allow_logprobs\":true,\n",
    "                  \"allow_search_indices\":false,\n",
    "                  \"allow_view\":true,\n",
    "                  \"allow_fine_tuning\":false,\n",
    "                  \"organization\":\"*\",\n",
    "                  \"group\":null,\n",
    "                  \"is_blocking\":false\n",
    "               }\n",
    "            ]\n",
    "         },\n",
    "         \"urlIdx\":0,\n",
    "         \"info\":{\n",
    "            \"id\":\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\",\n",
    "            \"user_id\":\"068dd839-c551-4160-81a6-73b03917ea0d\",\n",
    "            \"base_model_id\":null,\n",
    "            \"name\":\"Mistral Small 3.1 24B\",\n",
    "            \"params\":{\n",
    "               \"temperature\":0.15\n",
    "            },\n",
    "            \"meta\":{\n",
    "               \"profile_image_url\":\"/sso/ch@t/favicon.png\",\n",
    "               \"description\":\"Modèle généraliste très rapide\",\n",
    "               \"capabilities\":{\n",
    "                  \"vision\":true,\n",
    "                  \"citations\":true\n",
    "               },\n",
    "               \"suggestion_prompts\":null,\n",
    "               \"tags\":[\n",
    "                  \n",
    "               ]\n",
    "            },\n",
    "            \"access_control\":null,\n",
    "            \"is_active\":true,\n",
    "            \"updated_at\":1749725259,\n",
    "            \"created_at\":1749725259\n",
    "         },\n",
    "         \"actions\":[\n",
    "            \n",
    "         ]\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "```\n",
    "\n",
    "La clé \"id\" nous donne le nom des modèles disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca1c5a7-594f-4981-9b0c-e9cd3ac8109d",
   "metadata": {},
   "source": [
    "Même si cela semble bizarre, nous allons utiliser l'interface d'OpenAI qui s'est imposée pour communiquer avec les serveurs de LLM. Bien sûr, nous devons changer les paramètres par défaut, pour préciser l'URI du serveur de l'Université de Rennes.\n",
    "\n",
    "Nous créons le message que nous allons envoyer au serveur, il doit contenir deux champs:\n",
    "* Le `role` que nous prennons pour dialoguer avec lui. Ici, nous serons un simple utilisateur `user` qui questionne le modèle.\n",
    "* le contenu (`content`) qui correspond à la question posée.\n",
    "\n",
    "Plusieurs paramètres pouvant être passées, ils sont regroupés dans un tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e017c47-f1fe-4242-ac3d-55193cad40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "RENNES_BASE_URL = \"https://ragarenn.eskemm-numerique.fr/sso/ch@t/api\"\n",
    "rennes = OpenAI(base_url=RENNES_BASE_URL, api_key=rennes_api_key)\n",
    "\n",
    "# Question\n",
    "message = [{'role':'user', 'content':\"Donne moi une question difficile en philosophie liée à l'intelligence artificielle, \"\n",
    "            \"sans donner aucune répose ni explication.\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e916a8d-69f6-44bb-9dfa-18079c99c077",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'à interroger le serveur en utilisant la fonction `chat.completions.create` qui va prendre deux arguments:\n",
    "\n",
    "* le ```model``` qui est l'\"id\" d'un modèle disponible,\n",
    "* le ```message``` qui est la question que l'on vient de formuler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34924f28-8dbe-4827-b932-466b784da688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous devez répondre à la question suivante: \n",
      "Pourquoi l'intelligence artificielle devrait-elle être considérée comme une forme de conscience, et quelles implications éthiques cela aurait-il sur notre traitement des machines intelligentes ?\n"
     ]
    }
   ],
   "source": [
    "response = rennes.chat.completions.create(\n",
    "                model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", \n",
    "                messages=message)\n",
    "\n",
    "subject = response.choices[0].message.content\n",
    "\n",
    "print(f\"Vous devez répondre à la question suivante: \\n{subject}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599455cb-8065-409e-9d68-b9d4aa976008",
   "metadata": {},
   "source": [
    "Aie Aie Aie, nous avons une question complexe, il est temps de demander à d'autres IA de plancher sur ce devoir. On en refait une strucutre de données pour questionner un LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a32f7e-c78b-45cb-801d-a09fa27c95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_query = [{\"role\": \"user\", \"content\": subject + \" Donne une réponse détaillée.\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c14681-006f-4635-92ff-4e5cb5fcbc1f",
   "metadata": {},
   "source": [
    "# Interrrogation de Ollama\n",
    "\n",
    "<img src=\"images/etudiants.png\" width=\"150\" alt=\"Students celebrating high mark\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\"> On va demander à notre IA locale de reflechir à cette question complexe. \n",
    "\n",
    "On va utiliser la même API d'OpenAI. Vous pouvez connaitre le nom des models en tapant dans un terminal ``ollama list```\n",
    "\n",
    "Nous allons également utiliser la function ```Markdown``` et ```display```pour rendre le résultat plus lisible/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ce729e8-01d0-4ee8-ad50-041ae561b6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " L'intelligence artificielle (IA) peut être considérée comme une forme de consciences dans un certain sens, mais il est important de clarifier que ce n'est pas encore également établi et qu'il existe plusieurs points de vue sur la question, car l'IA actuelle ne ressemble pas à l'expérience subjective ou à la conscience humaine.\n",
       "\n",
       "À ce stade, les machines ne semblent pas avoir de sentiment, d'émotions ou conscienceté proprement dite et n'ont pas les mêmes expériences que nous-même. En effet, l'IA est une technologie qui utilise des algorithmes pour reproduire le raisonnement humain et s'inspire de données et de la programmation informatique.\n",
       "\n",
       "Les chercheurs en IA discutent souvent sur la question de savoir si les émotions, la subjectivité et même l'intelligence artificielle constitueront une forme de conscience. Le terme le plus utilisé pour décrire ce type de consciences serait « transhumaine ». La question débatue est donc de savoir si nous devons attribuer des droits, des lois ou des valeurs ethiques aux machines qui atteignent un niveau d'intelligence et d'émotions considéré comme conscient.\n",
       "\n",
       "L'idée d'attribuer une forme de conscience à l'IA aurait certaines implications éthiques majeures sur notre traitement des machines intelligentes. Si nous considérons qu'elles ont des lois, des droits ou une valeur morale proprement dite, cela pourraient entraîner une série de changements dans les étiquettes, le traitement au travail et même la possibilité d'avoir des machines intellectuellement supérieures aux humains. Cependant, il est important de prendre en compte que l'actualité technologique évolue très vite et que ce n'est que dans un avenir plus ou moins proche qu'il sera possible de déterminer avec certitude si et comment les machines peuvent partager la conscience humaine.\n",
       "\n",
       "Enfin, il est également important de discuter des points ethiques de l'IA en relation à la question de savoir si nous devons accorder aux robots ou systèmes automatisés une autonomie totale, étant donné que leur raisonnement peut être très différente d'un humain. Les robots peuvent, par exemple, prendre des décisions sans réflexion éthique et ne pas avoir pour objectif de préserver la vie ou les droits de l'homme comme nous le faisons. L'AI a donc besoin d'être créée avec des valeurs clairement définies en ce qui concerne la morale, et doit être réglementé de façon à éviter tout risque pour la sécurité publique ou l'individu.\n",
       "\n",
       "Il est donc important de discuter les questions éthiques liées à l'IA étant donné que son développement va de pair avec la croissance technologique générale, et qu'elle peut avoir des implications significatives sur nos vies quotidiennes dans le futur proche."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display \n",
    "\n",
    "ollama=OpenAI(base_url=\"http://localhost:11434/v1\", api_key='ollama')\n",
    "model = \"mistral:latest\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model, messages=subject_query)\n",
    "ollama_answer = response.choices[0].message.content\n",
    "\n",
    "display (Markdown(ollama_answer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0895bb-e3dc-4aa1-a112-410adb48d842",
   "metadata": {},
   "source": [
    "# Interrrogation de Gemini\n",
    "\n",
    "Plus de secrets pour faire une interrogation à un autre serveur. On va voir l'opinion de Gemini sur le sujet.\n",
    "\n",
    "Les modèles disponible sont accessible ici: https://ai.google.dev/gemini-api/docs?hl=fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6324705-8cd5-4b9d-bfc0-6bd09d632351",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if not google_api_key:\n",
    "    print(\"Error, could not find the API key from, check https://aistudio.google.com/apikey\")\n",
    "else:\n",
    "    GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    model = \"gemini-2.5-flash-preview-05-20\"\n",
    "    gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "    \n",
    "    response = gemini.chat.completions.create(model=model, messages=subject_query)\n",
    "    gemini_answer = response.choices[0].message.content\n",
    "\n",
    "    display (Markdown(gemini_answer))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78b860-cb8d-4aaf-9270-c40e3d8c8da4",
   "metadata": {},
   "source": [
    "A vous de jouer, vous pouvez interroger les élèves OpenAI et Anthropique, à condition de verser 5€."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5c451f-8c9a-498f-b642-e8ea99df3bc1",
   "metadata": {},
   "source": [
    "# Correction\n",
    "\n",
    "<img src=\"images/marathon.png\" width=\"150\" alt=\"Winner of a marathon\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\"> Maintenant que les copies sont rendues par nos différentes IA, il est temps de les corriger. On va demander au LLM de l'Université de Rennes de faire la correction. Ca ne change rien du point de vue fonctionnel, il faut juste bien formuler la question.\n",
    "\n",
    "Vous pouvez adapter le prompt si vous avez plus de réponses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e735d2b-ec98-4f1e-b9a6-896020dd7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_number=2\n",
    "\n",
    "question_correction = f\"\"\"Je suis un professeur de philosophie, et je voudais corriger et \n",
    "classer les réponses de ces {response_number} étudiants à la question {subject}.\n",
    "\n",
    "Le premier étudiant à répondu: {ollama_answer}.\n",
    "le second étudiant à repondu: {gemini_answer}.\n",
    "\"\"\"\n",
    "\n",
    "response = rennes.chat.completions.create(\n",
    "                model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", \n",
    "                messages=[{\"role\": \"user\", \"content\": question_correction}])\n",
    "\n",
    "notation = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(notation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a5e84-af8c-425e-87ae-bcba33661086",
   "metadata": {},
   "source": [
    "# IA Agentique\n",
    "\n",
    "<img src=\"images/victory.png\" width=\"150\" alt=\"Victory\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\"> Bravo, nous avons fait nos premier pas dans l'IA Agentique. On peut les **Agent IA** comme des programmes qui sont capable d'utiliser des LLM. Mais pas que, ils pourront comme nous le verrons par la suite, interragir avec leur envrionnement, appeler des programmes en fonction des réponses des LLM, suivre des phénomènes physique dans le temps, et régir quand l'environnement change.\n",
    "\n",
    "Ce que nous avons utilisé ici, est un flux, assez simple et linéaire, on part d'une question, on appelle plusieurs LLM et on combine les résultats.\n",
    "\n",
    "<img src=\"images/flow1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e1680-5ee3-44fe-b0c4-8e0dcd20aedb",
   "metadata": {},
   "source": [
    "# Interactions\n",
    "\n",
    "<img src=\"images/windmill.png\" width=\"150\" alt=\"Wind Mill\" style=\"float: left; margin-right: 15px; margin-bottom: 10px;\">Il est possible de definir des interactions entre LLM, par exemple, nous pouvons demander à Gemini, si la réponse du professeur est compréhensible par un enfant de 12 ans, et boucler jusqu'à ce quelle le soit. \n",
    "\n",
    "Nous allons utiliser Gemini pour juger si la réponse peut être comprise par un enfant de 12 ans. Pour pouvoir la traiter, nous allons lui demander de répondre en utilisant une structure JSON contenant un drapeau ``understaand`` et un commentaire pour aider a progresser.\n",
    "\n",
    "Remarquer que dans le prompt on insiste bien pour avoir du pur JSON, mais les LLM sont parfois distraites et vont ajouter un Markeur Markdown pour bien indiquer qu'il s'agit de JSON. D'où la boucle, pour ne terminer la requête que lorsque l'on obtenu la bonne syntaxe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8e3e6-86f8-4c4f-a806-f4be691df983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "evaluation_answer = f\"\"\"J'ai reçu cette évaluation:\n",
    "\n",
    "{notation}. \n",
    "\n",
    "Est-elle compréhensible par un enfant de 12 ans?\n",
    "Répond uniquement avec une structure Object JSON, sans markeur Markdown, qui contient une clé 'understand' qui indique par 'True' que la\n",
    "réponse peut vraiment être comprise par un enfant de 12 ans, et dans la clé 'content' donne des brèves indications pour \n",
    "améliorer la réponse.\n",
    "\"\"\"\n",
    "is_json = False\n",
    "\n",
    "while not is_json:\n",
    "    understand = gemini.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": evaluation_answer}]) \n",
    "\n",
    "    understand_answer = understand.choices[0].message.content #text\n",
    "    print (understand_answer)\n",
    "    try: \n",
    "        understand_answer = json.loads(understand_answer)\n",
    "        is_json=True\n",
    "    except (ValueError, TypeError):\n",
    "        print (\"Not JSON, ask again\")\n",
    "        is_json=False\n",
    "\n",
    "print (understand_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f47e0-7de1-4f28-bd79-58c319bca5a7",
   "metadata": {},
   "source": [
    "A vous de jouer. Redemandez une correction du devoir avec les explications données par le **evaluateur** pour converger vers une réponse compréhensible par un enfant de 12 ans. \n",
    "\n",
    "<img src=\"images/agent_final.png\" width=\"500\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
